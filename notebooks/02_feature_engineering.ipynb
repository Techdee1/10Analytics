{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/cleaned_panel_data.csv')\n",
    "\n",
    "# Load focus countries\n",
    "with open('../data/focus_countries.txt', 'r') as f:\n",
    "    focus_countries = f.read().strip().split(',')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Focus countries: {focus_countries}\")\n",
    "print(f\"\\nAvailable indicators: {[col for col in df.columns if col not in ['country', 'country_code', 'year']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73341e2d",
   "metadata": {},
   "source": [
    "## 1. Calculate Core Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Debt-to-GDP ratio (primary crisis indicator)\n",
    "if 'government_debt' in df_features.columns and 'nominal_gdp' in df_features.columns:\n",
    "    df_features['debt_to_gdp'] = (df_features['government_debt'] / df_features['nominal_gdp']) * 100\n",
    "    print(\"✅ Created: debt_to_gdp\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Cannot calculate debt_to_gdp - missing debt or GDP data\")\n",
    "\n",
    "# 2. Budget deficit as % of GDP\n",
    "if 'budget_deficit_surplus' in df_features.columns and 'nominal_gdp' in df_features.columns:\n",
    "    df_features['deficit_to_gdp'] = (df_features['budget_deficit_surplus'] / df_features['nominal_gdp']) * 100\n",
    "    print(\"✅ Created: deficit_to_gdp\")\n",
    "\n",
    "# 3. Revenue as % of GDP (revenue mobilization capacity)\n",
    "if 'revenue' in df_features.columns and 'nominal_gdp' in df_features.columns:\n",
    "    df_features['revenue_to_gdp'] = (df_features['revenue'] / df_features['nominal_gdp']) * 100\n",
    "    print(\"✅ Created: revenue_to_gdp\")\n",
    "\n",
    "# 4. Expenditure as % of GDP\n",
    "if 'expenditure' in df_features.columns and 'nominal_gdp' in df_features.columns:\n",
    "    df_features['expenditure_to_gdp'] = (df_features['expenditure'] / df_features['nominal_gdp']) * 100\n",
    "    print(\"✅ Created: expenditure_to_gdp\")\n",
    "\n",
    "# 5. Trade balance as % of GDP\n",
    "if 'exports' in df_features.columns and 'imports' in df_features.columns and 'nominal_gdp' in df_features.columns:\n",
    "    df_features['trade_balance'] = df_features['exports'] - df_features['imports']\n",
    "    df_features['trade_balance_to_gdp'] = (df_features['trade_balance'] / df_features['nominal_gdp']) * 100\n",
    "    print(\"✅ Created: trade_balance, trade_balance_to_gdp\")\n",
    "\n",
    "# 6. Tax revenue efficiency\n",
    "if 'tax_revenue' in df_features.columns and 'revenue' in df_features.columns:\n",
    "    df_features['tax_revenue_share'] = (df_features['tax_revenue'] / df_features['revenue']) * 100\n",
    "    print(\"✅ Created: tax_revenue_share\")\n",
    "\n",
    "print(f\"\\nNew feature count: {len([c for c in df_features.columns if c not in df.columns])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00c6fc",
   "metadata": {},
   "source": [
    "## 2. Create Debt Crisis Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define debt crisis based on multiple criteria\n",
    "# Criteria based on IMF and World Bank thresholds for developing economies\n",
    "\n",
    "df_features['crisis_debt_threshold'] = 0  # Debt-to-GDP > 70%\n",
    "df_features['crisis_high_deficit'] = 0    # Deficit > 5% of GDP for sustained period\n",
    "df_features['crisis_composite'] = 0        # Combined crisis indicator\n",
    "\n",
    "# 1. High debt threshold (>70% for developing countries)\n",
    "if 'debt_to_gdp' in df_features.columns:\n",
    "    df_features.loc[df_features['debt_to_gdp'] > 70, 'crisis_debt_threshold'] = 1\n",
    "    debt_crisis_count = df_features['crisis_debt_threshold'].sum()\n",
    "    print(f\"High debt periods (>70% GDP): {debt_crisis_count} ({debt_crisis_count/len(df_features)*100:.1f}%)\")\n",
    "\n",
    "# 2. Severe deficit (deficit > 5% of GDP)\n",
    "if 'deficit_to_gdp' in df_features.columns:\n",
    "    df_features.loc[df_features['deficit_to_gdp'] < -5, 'crisis_high_deficit'] = 1\n",
    "    deficit_crisis_count = df_features['crisis_high_deficit'].sum()\n",
    "    print(f\"High deficit periods (>5% GDP): {deficit_crisis_count} ({deficit_crisis_count/len(df_features)*100:.1f}%)\")\n",
    "\n",
    "# 3. Composite crisis indicator (either condition triggers crisis)\n",
    "df_features['crisis_composite'] = ((df_features['crisis_debt_threshold'] == 1) | \n",
    "                                    (df_features['crisis_high_deficit'] == 1)).astype(int)\n",
    "\n",
    "composite_crisis_count = df_features['crisis_composite'].sum()\n",
    "print(f\"\\nComposite crisis periods: {composite_crisis_count} ({composite_crisis_count/len(df_features)*100:.1f}%)\")\n",
    "\n",
    "# Show crisis distribution by country\n",
    "print(\"\\nCrisis distribution by country:\")\n",
    "crisis_summary = df_features.groupby('country').agg({\n",
    "    'crisis_composite': ['sum', 'mean'],\n",
    "    'debt_to_gdp': 'mean'\n",
    "}).round(2)\n",
    "crisis_summary.columns = ['Crisis_Count', 'Crisis_Rate', 'Avg_Debt_GDP']\n",
    "print(crisis_summary.sort_values('Crisis_Rate', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9bdc7",
   "metadata": {},
   "source": [
    "## 3. Create Temporal Features (Trends & Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c70f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by country and year for time-series operations\n",
    "df_features = df_features.sort_values(['country', 'year'])\n",
    "\n",
    "# Function to create lagged and rolling features\n",
    "def create_temporal_features(df, country_col='country', year_col='year', window=3):\n",
    "    \"\"\"\n",
    "    Create temporal features: lags, changes, rolling statistics\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Key indicators for temporal features\n",
    "    temporal_cols = ['debt_to_gdp', 'deficit_to_gdp', 'gdp_growth_rate', \n",
    "                     'inflation_rate', 'revenue_to_gdp', 'trade_balance_to_gdp']\n",
    "    \n",
    "    # Filter to available columns\n",
    "    temporal_cols = [col for col in temporal_cols if col in df_temp.columns]\n",
    "    \n",
    "    for col in temporal_cols:\n",
    "        # 1-year change\n",
    "        df_temp[f'{col}_change_1y'] = df_temp.groupby(country_col)[col].diff(1)\n",
    "        \n",
    "        # 3-year change\n",
    "        df_temp[f'{col}_change_3y'] = df_temp.groupby(country_col)[col].diff(3)\n",
    "        \n",
    "        # Rolling average (3-year)\n",
    "        df_temp[f'{col}_rolling_avg'] = df_temp.groupby(country_col)[col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Rolling standard deviation (volatility)\n",
    "        df_temp[f'{col}_volatility'] = df_temp.groupby(country_col)[col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=2).std()\n",
    "        )\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "print(\"Creating temporal features...\")\n",
    "df_features = create_temporal_features(df_features)\n",
    "\n",
    "# Count new features\n",
    "new_features = [c for c in df_features.columns if any(x in c for x in ['_change_', '_rolling_', '_volatility'])]\n",
    "print(f\"✅ Created {len(new_features)} temporal features\")\n",
    "print(f\"Examples: {new_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06241a6e",
   "metadata": {},
   "source": [
    "## 4. Create Deficit Persistence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count consecutive deficit years (indicator of fiscal stress)\n",
    "def count_consecutive_deficits(group):\n",
    "    \"\"\"\n",
    "    Count consecutive years of budget deficits\n",
    "    \"\"\"\n",
    "    if 'deficit_to_gdp' not in group.columns:\n",
    "        return pd.Series(0, index=group.index)\n",
    "    \n",
    "    is_deficit = (group['deficit_to_gdp'] < 0).astype(int)\n",
    "    \n",
    "    # Calculate consecutive count\n",
    "    consecutive = []\n",
    "    count = 0\n",
    "    for val in is_deficit:\n",
    "        if val == 1:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        consecutive.append(count)\n",
    "    \n",
    "    return pd.Series(consecutive, index=group.index)\n",
    "\n",
    "if 'deficit_to_gdp' in df_features.columns:\n",
    "    df_features['consecutive_deficit_years'] = df_features.groupby('country').apply(\n",
    "        count_consecutive_deficits\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    \n",
    "    print(\"✅ Created: consecutive_deficit_years\")\n",
    "    print(f\"Max consecutive deficits: {df_features['consecutive_deficit_years'].max():.0f} years\")\n",
    "    print(f\"Countries with 5+ consecutive deficits: {df_features[df_features['consecutive_deficit_years'] >= 5]['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755509ee",
   "metadata": {},
   "source": [
    "## 5. Feature Summary & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30525ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all feature columns (excluding identifiers and labels)\n",
    "identifier_cols = ['country', 'country_code', 'year']\n",
    "label_cols = ['crisis_debt_threshold', 'crisis_high_deficit', 'crisis_composite']\n",
    "original_indicators = [c for c in df.columns if c not in identifier_cols]\n",
    "\n",
    "# All feature columns\n",
    "all_features = [c for c in df_features.columns if c not in identifier_cols + label_cols]\n",
    "\n",
    "print(f\"Total feature count: {len(all_features)}\")\n",
    "print(f\"  - Original indicators: {len(original_indicators)}\")\n",
    "print(f\"  - Engineered features: {len(all_features) - len(original_indicators)}\")\n",
    "\n",
    "# Check for features with too many missing values\n",
    "missing_pct = df_features[all_features].isnull().sum() / len(df_features) * 100\n",
    "high_missing = missing_pct[missing_pct > 70].sort_values(ascending=False)\n",
    "\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"\\n⚠️ Features with >70% missing values ({len(high_missing)}):\")\n",
    "    print(high_missing.head(10))\n",
    "\n",
    "# Select features with reasonable coverage (<70% missing)\n",
    "usable_features = missing_pct[missing_pct <= 70].index.tolist()\n",
    "print(f\"\\n✅ Usable features: {len(usable_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47245f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature coverage\n",
    "plt.figure(figsize=(14, 8))\n",
    "coverage = (100 - missing_pct[usable_features]).sort_values(ascending=True)\n",
    "coverage.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Data Coverage (%)', fontsize=11)\n",
    "plt.ylabel('Feature', fontsize=11)\n",
    "plt.title('Feature Data Coverage', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=50, color='red', linestyle='--', label='50% threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFeatures with >50% coverage: {len(coverage[coverage > 50])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83784148",
   "metadata": {},
   "source": [
    "## 6. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-ready dataset\n",
    "ml_data = df_features[identifier_cols + usable_features + label_cols].copy()\n",
    "\n",
    "print(f\"ML dataset shape: {ml_data.shape}\")\n",
    "print(f\"Missing values before imputation:\")\n",
    "print(ml_data[usable_features].isnull().sum().sum(), \"total missing values\")\n",
    "\n",
    "# Strategy: Forward fill within each country (assumes recent values are relevant)\n",
    "# Then backward fill if needed\n",
    "for country in ml_data['country'].unique():\n",
    "    mask = ml_data['country'] == country\n",
    "    ml_data.loc[mask, usable_features] = ml_data.loc[mask, usable_features].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# For any remaining NaNs, fill with median (conservative approach)\n",
    "for col in usable_features:\n",
    "    if ml_data[col].isnull().any():\n",
    "        median_val = ml_data[col].median()\n",
    "        ml_data[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {ml_data[usable_features].isnull().sum().sum()}\")\n",
    "print(\"✅ Missing value imputation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c44519",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440088ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where target variable is NaN\n",
    "ml_data_clean = ml_data.dropna(subset=['crisis_composite'])\n",
    "\n",
    "print(f\"Clean dataset shape: {ml_data_clean.shape}\")\n",
    "print(f\"Crisis rate in dataset: {ml_data_clean['crisis_composite'].mean()*100:.1f}%\")\n",
    "\n",
    "# Temporal split: train on earlier years, test on recent years\n",
    "split_year = ml_data_clean['year'].quantile(0.75)  # Use 75th percentile as split\n",
    "\n",
    "train_data = ml_data_clean[ml_data_clean['year'] < split_year]\n",
    "test_data = ml_data_clean[ml_data_clean['year'] >= split_year]\n",
    "\n",
    "print(f\"\\nTemporal split at year: {split_year:.0f}\")\n",
    "print(f\"Train set: {len(train_data)} observations ({train_data['year'].min():.0f}-{train_data['year'].max():.0f})\")\n",
    "print(f\"Test set: {len(test_data)} observations ({test_data['year'].min():.0f}-{test_data['year'].max():.0f})\")\n",
    "\n",
    "print(f\"\\nCrisis rate in train set: {train_data['crisis_composite'].mean()*100:.1f}%\")\n",
    "print(f\"Crisis rate in test set: {test_data['crisis_composite'].mean()*100:.1f}%\")\n",
    "\n",
    "# Check country distribution\n",
    "print(f\"\\nCountries in train: {train_data['country'].nunique()}\")\n",
    "print(f\"Countries in test: {test_data['country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f782ba",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = train_data[usable_features]\n",
    "y_train = train_data['crisis_composite']\n",
    "X_test = test_data[usable_features]\n",
    "y_test = test_data['crisis_composite']\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=usable_features, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=usable_features, index=X_test.index)\n",
    "\n",
    "print(\"✅ Feature scaling complete\")\n",
    "print(f\"\\nScaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc360122",
   "metadata": {},
   "source": [
    "## 9. Export Feature-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete feature dataset\n",
    "ml_data_clean.to_csv('../data/ml_features_complete.csv', index=False)\n",
    "print(\"✅ Saved: ml_features_complete.csv\")\n",
    "\n",
    "# Save train/test splits (unscaled)\n",
    "train_data.to_csv('../data/ml_train.csv', index=False)\n",
    "test_data.to_csv('../data/ml_test.csv', index=False)\n",
    "print(\"✅ Saved: ml_train.csv, ml_test.csv\")\n",
    "\n",
    "# Save scaled versions\n",
    "X_train_scaled.to_csv('../data/ml_X_train_scaled.csv', index=True)\n",
    "X_test_scaled.to_csv('../data/ml_X_test_scaled.csv', index=True)\n",
    "y_train.to_csv('../data/ml_y_train.csv', index=True, header=True)\n",
    "y_test.to_csv('../data/ml_y_test.csv', index=True, header=True)\n",
    "print(\"✅ Saved: ml_X_train_scaled.csv, ml_X_test_scaled.csv, ml_y_train.csv, ml_y_test.csv\")\n",
    "\n",
    "# Save feature list\n",
    "with open('../data/feature_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(usable_features))\n",
    "print(\"✅ Saved: feature_list.txt\")\n",
    "\n",
    "# Save scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/feature_scaler.pkl')\n",
    "print(\"✅ Saved: feature_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622af133",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATASET:\")\n",
    "print(f\"   Total observations: {len(ml_data_clean)}\")\n",
    "print(f\"   Training set: {len(train_data)} ({len(train_data)/len(ml_data_clean)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_data)} ({len(test_data)/len(ml_data_clean)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. FEATURES:\")\n",
    "print(f\"   Total features: {len(usable_features)}\")\n",
    "print(f\"   - Core ratios: ~8 (debt-to-GDP, deficit-to-GDP, etc.)\")\n",
    "print(f\"   - Temporal features: {len([f for f in usable_features if any(x in f for x in ['change', 'rolling', 'volatility'])])}\")\n",
    "print(f\"   - Original indicators: {len([f for f in usable_features if f in original_indicators])}\")\n",
    "\n",
    "print(f\"\\n3. TARGET VARIABLE (Crisis):\")\n",
    "print(f\"   Crisis definition: Debt-to-GDP >70% OR Deficit >5% of GDP\")\n",
    "print(f\"   Overall crisis rate: {ml_data_clean['crisis_composite'].mean()*100:.1f}%\")\n",
    "print(f\"   Train crisis rate: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"   Test crisis rate: {y_test.mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n4. DATA QUALITY:\")\n",
    "print(f\"   Features with >50% coverage: {len([f for f in usable_features if (100 - missing_pct[f]) > 50])}\")\n",
    "print(f\"   Missing values imputed: Yes (forward/backward fill + median)\")\n",
    "print(f\"   Feature scaling: StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "print(f\"\\n5. KEY FEATURES FOR MODELING:\")\n",
    "# List top features by coverage\n",
    "top_features = (100 - missing_pct[usable_features]).sort_values(ascending=False).head(15)\n",
    "for i, (feat, cov) in enumerate(top_features.items(), 1):\n",
    "    print(f\"   {i:2d}. {feat:40s} ({cov:5.1f}% coverage)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Feature Engineering Complete! Ready for ML Model Training\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505aaa5",
   "metadata": {},
   "source": [
    "## ✅ Feature Engineering Complete!\n",
    "\n",
    "### What Was Done:\n",
    "1. **Core Ratios Created**: Debt-to-GDP, deficit-to-GDP, revenue-to-GDP, expenditure-to-GDP, trade-balance-to-GDP\n",
    "2. **Crisis Labels Defined**: Debt >70% GDP OR Deficit >5% GDP\n",
    "3. **Temporal Features**: 1-year & 3-year changes, rolling averages, volatility measures\n",
    "4. **Persistence Features**: Consecutive deficit years tracking\n",
    "5. **Missing Value Handling**: Forward/backward fill + median imputation\n",
    "6. **Train-Test Split**: Temporal split (75/25) to avoid data leakage\n",
    "7. **Feature Scaling**: StandardScaler for normalized inputs\n",
    "\n",
    "### Outputs:\n",
    "- `ml_features_complete.csv` - Full feature dataset\n",
    "- `ml_train.csv` / `ml_test.csv` - Train/test splits\n",
    "- `ml_X_train_scaled.csv` / `ml_X_test_scaled.csv` - Scaled features\n",
    "- `ml_y_train.csv` / `ml_y_test.csv` - Target labels\n",
    "- `feature_list.txt` - List of all features\n",
    "- `feature_scaler.pkl` - Fitted scaler for deployment\n",
    "\n",
    "### Next Step:\n",
    "**ML Model Training** (`03_ml_debt_crisis.ipynb`)\n",
    "- Random Forest Classifier\n",
    "- XGBoost Classifier\n",
    "- Logistic Regression\n",
    "- Model comparison & evaluation\n",
    "- Feature importance analysis\n",
    "- Country risk score generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
